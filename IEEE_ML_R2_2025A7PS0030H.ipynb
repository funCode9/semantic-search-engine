{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/funCode9/semantic-search-engine/blob/main/IEEE_ML_R2_2025A7PS0030H.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmZBuRC7rEX_",
        "outputId": "ae77a994-7395-43c5-9d65-50f9c46332f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyIHtKdvrOmD",
        "outputId": "ced36920-c836-4d4f-cc30-517bda48f92f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ieee_ml\n",
            "collection.tsv\n",
            "queries.dev.tsv\n",
            "queries.eval.tsv\n",
            "queries.train.tsv\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/ieee_ml\n",
        "!tar -xvzf collection.tar.gz\n",
        "!tar -xvzf queries.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKLzu5V9d01W",
        "outputId": "2adec82b-3213-410c-ab10-6d99b2acdf7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "building realistic system where 1396 queries out of 6980 will have an answer.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import csv\n",
        "\n",
        "queries_df = pd.read_csv('queries.dev.small.tsv', sep='\\t', names=['qid', 'text'], dtype={'qid': str})\n",
        "qrels_df = pd.read_csv('qrels.dev.small.tsv', sep='\\t', names=['qid', 'fixed', 'pid', 'rel'], dtype={'qid': str, 'pid': str})\n",
        "\n",
        "all_qids = list(queries_df['qid'].unique())\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "# 0.20 becuase in realistic dataset not all queries have the correct answer at the top, it is to ensure recall is not too high artificially.\n",
        "answerable_qids = set(random.sample(all_qids, int(len(all_qids) * 0.20)))\n",
        "answerable_pids = set(qrels_df[qrels_df['qid'].isin(answerable_qids)]['pid'].astype(str))\n",
        "\n",
        "print(f\"building realistic system where {len(answerable_qids)} queries out of {len(all_qids)} will have an answer.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYXvPmhWdxm0",
        "outputId": "b89e959e-e6a0-45f4-efcd-99d86ca3ae53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subset Created: 100000 passages.\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import csv\n",
        "import pandas as pd\n",
        "\n",
        "subset_passages = []\n",
        "target_size = 100000\n",
        "\n",
        "# probability = approx 0.012 = 100k/8.8M\n",
        "\n",
        "sampling_probability = 0.012\n",
        "\n",
        "with open('/content/drive/MyDrive/ieee_ml/collection.tsv', 'r', encoding='utf-8') as f:\n",
        "    reader = csv.reader(f, delimiter='\\t')\n",
        "    grab_next_count = 0\n",
        "\n",
        "    for row in reader:\n",
        "        if len(row) < 2:\n",
        "            continue\n",
        "        pid = str(row[0])\n",
        "\n",
        "        # including gold answers\n",
        "        if pid in answerable_pids:\n",
        "            subset_passages.append({'pid': pid, 'text': row[1]})\n",
        "            grab_next_count = 3 # Tell the loop to grab the next 3 rows\n",
        "            continue\n",
        "\n",
        "        # including neighbor negatives (Hard negatives)\n",
        "        if grab_next_count > 0 and len(subset_passages) < target_size:\n",
        "            subset_passages.append({'pid': pid, 'text': row[1]})\n",
        "            grab_next_count -= 1\n",
        "            continue\n",
        "\n",
        "        # Random sampling (filling up to reach 100k subset)\n",
        "        if len(subset_passages) < target_size:\n",
        "            if random.random() < sampling_probability:\n",
        "                subset_passages.append({'pid': pid, 'text': row[1]})\n",
        "\n",
        "# trimming subset to 100k if it went beyond it\n",
        "if len(subset_passages) > target_size:\n",
        "    subset_passages = subset_passages[:target_size]\n",
        "\n",
        "subset_df = pd.DataFrame(subset_passages)\n",
        "subset_df['pid'] = subset_df['pid'].astype(str)\n",
        "subset_df.to_csv('subset_collection.csv', index=False)\n",
        "print(f\"Subset Created: {len(subset_df)} passages.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OzV8-pnFt_fe",
        "outputId": "287858e4-67a7-499d-c51c-66ba21026a05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.2.2)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.13.2)\n",
            "Requirement already satisfied: transformers<6.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (5.0.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.3.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.9.0+cu126)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (0.28.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.5.4)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (0.21.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (0.16.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->huggingface-hub>=0.20.0->sentence-transformers) (8.3.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
            "Key                     | Status     |  | \n",
            "------------------------+------------+--+-\n",
            "embeddings.position_ids | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded on: cuda\n"
          ]
        }
      ],
      "source": [
        "!pip install -U sentence-transformers faiss-cpu\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
        "\n",
        "print(f\"Model loaded on: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "id": "P73VC8LFvi3Y",
        "outputId": "0a72b8ff-d270-416e-8c4f-130f395bc73c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding 100k passages\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total passages indexed: 100000\n"
          ]
        }
      ],
      "source": [
        "# Loading and encoding 100k subset\n",
        "subset_df = pd.read_csv('subset_collection.csv')\n",
        "\n",
        "print(\"Encoding 100k passages\")\n",
        "passage_embeddings = model.encode(subset_df['text'].tolist(), batch_size=128, show_progress_bar=True, convert_to_numpy=True)\n",
        "\n",
        "# Initializing FAISS Index\n",
        "# MiniLM uses 384 integers to code sentence into a vector\n",
        "dimension = passage_embeddings.shape[1]\n",
        "index = faiss.IndexFlatIP(dimension)\n",
        "\n",
        "# Normalizing vectors and adding to Index\n",
        "faiss.normalize_L2(passage_embeddings)\n",
        "index.add(passage_embeddings.astype('float32'))\n",
        "\n",
        "print(f\"Total passages indexed: {index.ntotal}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82ctVxMreYDO",
        "outputId": "8d2654b6-678a-46a5-c78d-408382ebc70d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating all 6980 queries...\n",
            "\n",
            "--- FINAL BASELINE METRICS ---\n",
            "MRR@10:       0.1661\n",
            "NDCG@10:      0.1769\n",
            "Recall@10:    0.2096\n",
            "Precision@10: 0.0218\n"
          ]
        }
      ],
      "source": [
        "# evaluation metrics (BASELINE MODEL = dense embedding)\n",
        "\n",
        "import math\n",
        "\n",
        "metrics = {\n",
        "    'mrr': 0,\n",
        "    'ndcg_10': 0,\n",
        "    'recall_10': 0,\n",
        "    'precision_10': 0\n",
        "}\n",
        "\n",
        "# Creating the Answer Key (mapping Query ID to a list of correct Passage IDs)\n",
        "relevant_map = qrels_df.groupby('qid')['pid'].apply(list).to_dict()\n",
        "\n",
        "total_queries = len(queries_df)\n",
        "\n",
        "print(f\"Evaluating all {total_queries} queries...\")\n",
        "\n",
        "for _, row in queries_df.iterrows():\n",
        "    qid, q_text = row['qid'], row['text']\n",
        "\n",
        "    # pids of gold answers\n",
        "    target_pids = relevant_map.get(qid, [])\n",
        "\n",
        "    # Searching top 10 results\n",
        "    q_emb = model.encode([q_text]).astype('float32')\n",
        "    faiss.normalize_L2(q_emb)\n",
        "    distances, indices = index.search(q_emb, k=10)\n",
        "\n",
        "    found_at_rank = -1\n",
        "    num_relevant_found = 0\n",
        "\n",
        "    # Calculating metrics for top 10 results\n",
        "    for rank, idx in enumerate(indices[0]):\n",
        "        retrieved_pid = str(subset_df.iloc[idx]['pid'])\n",
        "\n",
        "        if retrieved_pid in target_pids:\n",
        "            num_relevant_found += 1\n",
        "            if found_at_rank == -1:\n",
        "                found_at_rank = rank\n",
        "\n",
        "                # MRR calculation\n",
        "                metrics['mrr'] += 1 / (rank + 1)\n",
        "\n",
        "                # NDCG calculation (using log2 for position discount)\n",
        "                metrics['ndcg_10'] += 1 / math.log2(rank + 2)\n",
        "\n",
        "    # Recall\n",
        "    if found_at_rank != -1:\n",
        "        metrics['recall_10'] += 1\n",
        "\n",
        "    # Precision - proportion of top 10 that were relevant\n",
        "    metrics['precision_10'] += num_relevant_found / 10\n",
        "\n",
        "# Final Average over ALL queries\n",
        "print(f\"\\n--- FINAL BASELINE METRICS ---\")\n",
        "print(f\"MRR@10:       {metrics['mrr'] / total_queries:.4f}\")\n",
        "print(f\"NDCG@10:      {metrics['ndcg_10'] / total_queries:.4f}\")\n",
        "print(f\"Recall@10:    {metrics['recall_10'] / total_queries:.4f}\")\n",
        "print(f\"Precision@10: {metrics['precision_10'] / total_queries:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dX67Rvc6yAo",
        "outputId": "43fa0300-db9e-4e3c-989a-bd3d9f160efc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subset built: 100000 passages\n"
          ]
        }
      ],
      "source": [
        "all_gold_pids = set(qrels_df['pid'].astype(str))\n",
        "\n",
        "subset_passages = []\n",
        "target_size = 100000\n",
        "\n",
        "# probability 100k / 8.8M lines ~ 0.0113.\n",
        "sampling_probability = 0.012\n",
        "\n",
        "with open('/content/drive/MyDrive/ieee_ml/collection.tsv', 'r', encoding='utf-8') as f:\n",
        "\n",
        "    reader = csv.reader(f, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
        "\n",
        "    for row in reader:\n",
        "        if len(row) < 2:\n",
        "          continue\n",
        "        pid = str(row[0])\n",
        "\n",
        "        # including gold answers\n",
        "        if pid in all_gold_pids:\n",
        "            subset_passages.append({'pid': pid, 'text': row[1]})\n",
        "\n",
        "        # random sampling for the rest\n",
        "        # We check probability FIRST to ensure we scan the whole file roughly evenly\n",
        "        elif len(subset_passages) < target_size and random.random() < sampling_probability:\n",
        "            subset_passages.append({'pid': pid, 'text': row[1]})\n",
        "\n",
        "\n",
        "subset_df = pd.DataFrame(subset_passages)\n",
        "subset_df = subset_df.drop_duplicates(subset='pid')\n",
        "\n",
        "# if subset is over 100k (over sampled), trimming it to 100k again\n",
        "if len(subset_df) > target_size:\n",
        "    subset_df = subset_df.sample(n=target_size, random_state=42)\n",
        "\n",
        "subset_df.to_csv('subset_collection.csv', index=False)\n",
        "print(f\"Subset built: {len(subset_df)} passages\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "referenced_widgets": []
        },
        "id": "RR7TBykS692b",
        "outputId": "f1bb8fef-b49c-4dd5-98e6-dc043a1fa8f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "BertForSequenceClassification LOAD REPORT from: cross-encoder/ms-marco-MiniLM-L-6-v2\n",
            "Key                          | Status     |  | \n",
            "-----------------------------+------------+--+-\n",
            "bert.embeddings.position_ids | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 384, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 384)\n",
              "      (token_type_embeddings): Embedding(2, 384)\n",
              "      (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-5): 6 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
              "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
              "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
              "              (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
              "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=384, out_features=384, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=384, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
        "import torch\n",
        "import faiss\n",
        "\n",
        "bi_model = SentenceTransformer('sentence-transformers/msmarco-distilbert-base-v4')\n",
        "cross_model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2', device='cuda')\n",
        "\n",
        "# Half-precision (from 32-bit to 16-int) increases speed to 2x\n",
        "cross_model.model.half()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "id": "_hQ7OgWl6_jX",
        "outputId": "b4f1f4fa-6f8b-4742-c667-d7246aa9b600"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding passages\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FAISS Index initialized successfully, Vector Dimension: 768\n"
          ]
        }
      ],
      "source": [
        "import faiss\n",
        "\n",
        "# Building FAISS Index\n",
        "passages = subset_df['text'].astype(str).tolist()\n",
        "\n",
        "print(\"Encoding passages\")\n",
        "expert_embeddings = bi_model.encode(passages, batch_size=128, show_progress_bar=True, convert_to_numpy=True)\n",
        "dimension = expert_embeddings.shape[1]\n",
        "\n",
        "# Normalizing for Cosine Similarity\n",
        "faiss.normalize_L2(expert_embeddings)\n",
        "\n",
        "expert_index = faiss.IndexFlatIP(dimension)\n",
        "expert_index.add(expert_embeddings.astype('float32'))\n",
        "\n",
        "# Creating the fast lookup dictionary\n",
        "pid_to_text = dict(zip(subset_df['pid'].astype(str), subset_df['text']))\n",
        "\n",
        "print(f\"FAISS Index initialized successfully, Vector Dimension: {dimension}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "cirj7Zf77BFw"
      },
      "outputs": [],
      "source": [
        "# Query Expansion\n",
        "def expand_query_pseudo_relevance(query, bi_model, index, subset_df, k=3):\n",
        "    q_emb = bi_model.encode([query]).astype('float32')\n",
        "    faiss.normalize_L2(q_emb)\n",
        "    _, indices = index.search(q_emb, k=k)\n",
        "    expansion_terms = []\n",
        "    for idx in indices[0]:\n",
        "        if 0 <= idx < len(subset_df):\n",
        "            text = subset_df.iloc[idx]['text']\n",
        "            expansion_terms.extend(text.split()[:5])\n",
        "    return query + \" \" + \" \".join(list(set(expansion_terms)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "id": "J8eRCFlAK8r3",
        "outputId": "bed50a02-2748-4685-d460-81b77554f7db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pre-computing lookups and optimizing models\n",
            "Starting evaluation on 6980 queries\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "FINAL RESULTS (Evaluated on 6980 queries)\n",
            "--------------------------------------------------\n",
            "MRR: 0.8654\n",
            "--------------------------------------------------\n",
            "k     | NDCG       | Recall     | Precision \n",
            "--------------------------------------------------\n",
            "1     | 0.8284     | 0.8074     | 0.8284    \n",
            "5     | 0.8722     | 0.9047     | 0.1899    \n",
            "10    | 0.8741     | 0.9099     | 0.0956    \n",
            "20    | 0.8744     | 0.9114     | 0.0479    \n",
            "100   | 0.8744     | 0.9114     | 0.0096    \n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "print(\"Pre-computing lookups and optimizing models\")\n",
        "pid_to_text = dict(zip(subset_df['pid'].astype(str), subset_df['text']))\n",
        "\n",
        "cross_model.model.half()\n",
        "\n",
        "K_LIST = [1, 5, 10, 20, 100]\n",
        "metrics = {'mrr': 0}\n",
        "for k in K_LIST:\n",
        "    metrics[f'ndcg@{k}'] = 0\n",
        "    metrics[f'recall@{k}'] = 0\n",
        "    metrics[f'precision@{k}'] = 0\n",
        "\n",
        "total_queries = len(queries_df)\n",
        "valid_queries_count = 0\n",
        "print(f\"Starting evaluation on {total_queries} queries\")\n",
        "\n",
        "for _, row in tqdm(queries_df.iterrows(), total=total_queries):\n",
        "    qid, q_text = row['qid'], row['text']\n",
        "    target_pids = set(relevant_map.get(qid, []))\n",
        "\n",
        "    if not target_pids:\n",
        "        continue\n",
        "\n",
        "    valid_queries_count += 1\n",
        "\n",
        "    # using expanded query\n",
        "    expanded_q_text = expand_query_pseudo_relevance(q_text, bi_model, expert_index, subset_df)\n",
        "\n",
        "    # STAGE 1: Dense Retrieval\n",
        "    q_emb = bi_model.encode([expanded_q_text], convert_to_numpy=True, show_progress_bar=False).astype('float32')\n",
        "    faiss.normalize_L2(q_emb)\n",
        "\n",
        "\n",
        "    # We retrieve 40 results\n",
        "    _, d_indices = expert_index.search(q_emb, k=40)\n",
        "\n",
        "    # Fast Dictionary Lookup\n",
        "    candidates = []\n",
        "    for idx in d_indices[0]:\n",
        "        if 0 <= idx < len(subset_df):\n",
        "            pid = str(subset_df.iloc[idx]['pid'])\n",
        "            if pid in pid_to_text:\n",
        "                candidates.append({'pid': pid, 'text': pid_to_text[pid]})\n",
        "\n",
        "    # STAGE 2: Re-ranking (Cross-Encoder)\n",
        "    if candidates:\n",
        "        model_inputs = [[q_text, c['text']] for c in candidates]\n",
        "        scores = cross_model.predict(model_inputs, batch_size=64, show_progress_bar=False)\n",
        "\n",
        "        for i in range(len(candidates)):\n",
        "            candidates[i]['score'] = scores[i]\n",
        "\n",
        "        reranked_candidates = sorted(candidates, key=lambda x: x['score'], reverse=True)\n",
        "    else:\n",
        "        reranked_candidates = []\n",
        "\n",
        "    # metric calculations\n",
        "\n",
        "    # 1. MRR\n",
        "    for rank, res in enumerate(reranked_candidates):\n",
        "        if res['pid'] in target_pids:\n",
        "            metrics['mrr'] += 1.0 / (rank + 1)\n",
        "            break\n",
        "\n",
        "    # 2. NDCG, Recall, Precision for all K\n",
        "    for k in K_LIST:\n",
        "        top_k = reranked_candidates[:k]\n",
        "        num_rel = sum(1 for res in top_k if res['pid'] in target_pids)\n",
        "\n",
        "        metrics[f'recall@{k}'] += num_rel / len(target_pids)\n",
        "        metrics[f'precision@{k}'] += num_rel / k\n",
        "\n",
        "        # NDCG Logic\n",
        "        dcg = 0\n",
        "        for rank, res in enumerate(top_k):\n",
        "            if res['pid'] in target_pids:\n",
        "                dcg += 1.0 / math.log2(rank + 2)\n",
        "\n",
        "        idcg = 0\n",
        "        for i in range(min(len(target_pids), k)):\n",
        "            idcg += 1.0 / math.log2(i + 2)\n",
        "\n",
        "        if idcg > 0:\n",
        "            metrics[f'ndcg@{k}'] += dcg / idcg\n",
        "\n",
        "# --- FINAL RESULTS ---\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(f\"FINAL RESULTS (Evaluated on {valid_queries_count} queries)\")\n",
        "print(\"-\"*50)\n",
        "print(f\"MRR: {metrics['mrr'] / valid_queries_count:.4f}\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"{'k':<5} | {'NDCG':<10} | {'Recall':<10} | {'Precision':<10}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for k in K_LIST:\n",
        "    ndcg = metrics[f'ndcg@{k}'] / valid_queries_count\n",
        "    rec = metrics[f'recall@{k}'] / valid_queries_count\n",
        "    prec = metrics[f'precision@{k}'] / valid_queries_count\n",
        "    print(f\"{k:<5} | {ndcg:<10.4f} | {rec:<10.4f} | {prec:<10.4f}\")\n",
        "print(\"-\"*50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "id": "tb_ZmwN5Bg_N",
        "outputId": "30ce6c83-9916-4207-9d76-3a0e05eaa3fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Efficiency Analysis on 100 queries\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "SYSTEM EFFICIENCY & LATENCY REPORT\n",
            "\n",
            "Metric          | Value       \n",
            "------------------------------\n",
            "Mean Latency    | 77.80 ms\n",
            "Median (P50)    | 76.89 ms\n",
            "P90 Latency     | 88.24 ms\n",
            "P95 Latency     | 91.50 ms\n",
            "P99 Latency     | 100.92 ms\n",
            "Std Dev         | 8.13 ms\n",
            "Throughput      | 13.01 QPS\n",
            "\n",
            "COMPONENT BREAKDOWN\n",
            "\n",
            "--------------------------------------------------\n",
            "Retrieval (Stage 1): 38.29 ms (49.2%)\n",
            "Re-ranking (Stage 2): 39.50 ms (50.8%)\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Settings\n",
        "num_bench_queries = 100\n",
        "latencies = {'total': [], 'retrieval': [], 'rerank': []}\n",
        "\n",
        "print(f\"Starting Efficiency Analysis on {num_bench_queries} queries\")\n",
        "\n",
        "for _ in tqdm(range(num_bench_queries)):\n",
        "\n",
        "    # Randomly sampling a query from the dataset\n",
        "    query_text = queries_df.sample(1)['text'].values[0]\n",
        "    start_total = time.time()\n",
        "\n",
        "    # STAGE 1: RETRIEVAL (Bi-Encoder + FAISS)\n",
        "    t1_start = time.time()\n",
        "    q_emb = bi_model.encode([query_text], convert_to_tensor=True, show_progress_bar=False).cpu().numpy().astype('float32')\n",
        "    faiss.normalize_L2(q_emb)\n",
        "    _, d_indices = expert_index.search(q_emb, k=40)\n",
        "\n",
        "    # Fast Dictionary Lookup\n",
        "    candidates_text = []\n",
        "\n",
        "    for idx in d_indices[0]:\n",
        "        if 0 <= idx < len(subset_df):\n",
        "            pid = str(subset_df.iloc[idx]['pid'])\n",
        "            text = pid_to_text[pid]\n",
        "            candidates_text.append(text)\n",
        "\n",
        "    latencies['retrieval'].append((time.time() - t1_start) * 1000)\n",
        "\n",
        "    # STAGE 2: RE-RANKING (Cross-Encoder)\n",
        "    t2_start = time.time()\n",
        "\n",
        "    if candidates_text:\n",
        "        model_inputs = [[query_text, text] for text in candidates_text]\n",
        "        _ = cross_model.predict(model_inputs, batch_size=64, show_progress_bar=False)\n",
        "\n",
        "    latencies['rerank'].append((time.time() - t2_start) * 1000)\n",
        "\n",
        "    latencies['total'].append((time.time() - start_total) * 1000)\n",
        "\n",
        "# CALCULATING METRICS\n",
        "def get_metrics(data):\n",
        "    return {\n",
        "        'mean': np.mean(data),\n",
        "        'p50': np.percentile(data, 50),\n",
        "        'p90': np.percentile(data, 90),\n",
        "        'p95': np.percentile(data, 95),\n",
        "        'p99': np.percentile(data, 99),\n",
        "        'std': np.std(data)\n",
        "    }\n",
        "\n",
        "total_m = get_metrics(latencies['total'])\n",
        "retrieval_m = get_metrics(latencies['retrieval'])\n",
        "rerank_m = get_metrics(latencies['rerank'])\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\nSYSTEM EFFICIENCY & LATENCY REPORT\\n\")\n",
        "print(f\"{'Metric':<15} | {'Value':<12}\")\n",
        "print(\"-\" * 30)\n",
        "print(f\"{'Mean Latency':<15} | {total_m['mean']:.2f} ms\")\n",
        "print(f\"{'Median (P50)':<15} | {total_m['p50']:.2f} ms\")\n",
        "print(f\"{'P90 Latency':<15} | {total_m['p90']:.2f} ms\")\n",
        "print(f\"{'P95 Latency':<15} | {total_m['p95']:.2f} ms\")\n",
        "print(f\"{'P99 Latency':<15} | {total_m['p99']:.2f} ms\")\n",
        "print(f\"{'Std Dev':<15} | {total_m['std']:.2f} ms\")\n",
        "print(f\"{'Throughput':<15} | {1000/total_m['p50']:.2f} QPS\")\n",
        "\n",
        "print(\"\\nCOMPONENT BREAKDOWN\\n\")\n",
        "print(\"-\"*50)\n",
        "print(f\"Retrieval (Stage 1): {retrieval_m['mean']:.2f} ms ({ (retrieval_m['mean']/total_m['mean'])*100:.1f}%)\")\n",
        "print(f\"Re-ranking (Stage 2): {rerank_m['mean']:.2f} ms ({ (rerank_m['mean']/total_m['mean'])*100:.1f}%)\")\n",
        "print(\"-\"*50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJpQhcjFBjaZ",
        "outputId": "ee9bbd2c-363e-4ba5-9441-b1c845dda167"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STORAGE & MEMORY ANALYSIS\n",
            "\n",
            "Embedding Index Size:  292.97 MB\n",
            "RAM for Embeddings:    292.97 MB\n",
            "Subset Text Size:      42.54 MB\n",
            "Total Model Memory (Runtime):    2.39 GB (GPU VRAM)\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "import torch\n",
        "\n",
        "# calculating FAISS index size on disk\n",
        "index_file = 'expert_index.faiss'\n",
        "faiss.write_index(expert_index, index_file)\n",
        "faiss_size_mb = os.path.getsize(index_file) / (1024 * 1024)\n",
        "\n",
        "# 2. Memory usage\n",
        "# Embeddings: 100,000 rows * 768 dims * 4 bytes per float\n",
        "emb_memory = (len(subset_df) * 768 * 4) / (1024 * 1024)\n",
        "\n",
        "# Convert bytes to GB, 1e9  = 10^9\n",
        "used_vram = torch.cuda.max_memory_allocated() / 1e9\n",
        "\n",
        "\n",
        "print(\"STORAGE & MEMORY ANALYSIS\\n\")\n",
        "print(f\"Embedding Index Size:  {faiss_size_mb:.2f} MB\")\n",
        "print(f\"RAM for Embeddings:     {emb_memory:.2f} MB\")\n",
        "print(f\"Subset Text Size:       {sys.getsizeof(subset_df) / (1024*1024):.2f} MB\")\n",
        "print(f\"Total Model Memory (Runtime):    {used_vram:.2f} GB (GPU VRAM)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ej7xGdM8HGy",
        "outputId": "a9814a22-8fca-4a47-eeee-0feb0bd0d27f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------\n",
            "MS MARCO NEURAL SEARCH SYSTEM\n",
            "Mode: Bi-Encoder Retrieval -> Cross-Encoder Re-Rank\n",
            "--------------------------------------------------\n",
            "\n",
            "------------------------------\n",
            "Enter Search Query (or 'exit' to quit): us population\n",
            "Enter Top-K results to return (default 10): 7\n",
            "Original Query: us population\n",
            "Expanded Query: us population is 31, It In 1790, population the Aug year of Current 2010 2011 from\n",
            "\n",
            "Top 7 Results for: 'us population'\n",
            "--------------------------------------------------\n",
            "Rank 1 | [PID: 6110584] | Score: 8.8281\n",
            "Text:  Aug 31, 2011 It is not difficult to find the information about how many people there are in the United States, because the U.S. Census Bureau1 regular...\n",
            "--------------------------------------------------\n",
            "Rank 2 | [PID: 1170798] | Score: 7.6523\n",
            "Text:  In 1790, the year of the first census of the U.S. population, there were 3,929,214 Americans. By 1900, the U.S.A. population jumped to 75,994,575. In ...\n",
            "--------------------------------------------------\n",
            "Rank 3 | [PID: 1230903] | Score: 5.8242\n",
            "Text:  There are 325,569,772 people in the United States of America. If everyone in the U.S. lined up single file, the line would stretch around the Earth al...\n",
            "--------------------------------------------------\n",
            "Rank 4 | [PID: 6266618] | Score: 4.5898\n",
            "Text:  With all of those combined, the U.S. is expected to add one person to the population every 17 seconds. The world population will increase even more, t...\n",
            "--------------------------------------------------\n",
            "Rank 5 | [PID: 3311200] | Score: 2.9531\n",
            "Text:  Population by Place in the United States There are 29,257 places in the United States. This section compares Attica to the 50 most populous places in ...\n",
            "--------------------------------------------------\n",
            "Rank 6 | [PID: 6263214] | Score: 1.9443\n",
            "Text:  Population Estimates: 2030: 38,538: 2025: 37,895: 2020: 37,101: 2015: 36,240: 2013: 33,685: 2010 Census Count: 33,736: 2005: 34,722: 2000 Census Count...\n",
            "--------------------------------------------------\n",
            "Rank 7 | [PID: 2130793] | Score: 1.7734\n",
            "Text:  United States and Texas Populations 1850-2016. Related Links. Texas Cities | Texas Counties | State Data Center | U.S. Bureau of the Census. From 1930...\n",
            "--------------------------------------------------\n",
            "\n",
            "------------------------------\n",
            "Enter Search Query (or 'exit' to quit): behaviour of cat\n",
            "Enter Top-K results to return (default 10): 6\n",
            "Original Query: behaviour of cat\n",
            "Expanded Query: behaviour of cat Much pet's your in cat's Any of mating change Behavior. Mating body Cat a\n",
            "\n",
            "Top 6 Results for: 'behaviour of cat'\n",
            "--------------------------------------------------\n",
            "Rank 1 | [PID: 1850477] | Score: 6.3750\n",
            "Text:  Cat Mating Behavior. Cat mating behavior starts with a female flirting with male cats (called toms). She will attract toms by vocalizing more (meowing...\n",
            "--------------------------------------------------\n",
            "Rank 2 | [PID: 1595432] | Score: 5.1445\n",
            "Text:  Much of a cat's body language is through its tail, ears, head position, and back posture. Cats flick their tails in an oscillating, snake-like motion,...\n",
            "--------------------------------------------------\n",
            "Rank 3 | [PID: 2591677] | Score: -0.4304\n",
            "Text:  Use food to motivate your cat. Cats respond to few things as well as food. If your cat has a fearful response to a friend or a new roommate who youâ\u0080\u0099...\n",
            "--------------------------------------------------\n",
            "Rank 4 | [PID: 7475706] | Score: -0.7666\n",
            "Text:  In fact, physical experiences play a vital role in how a cat recalls information long-term. By nature, cats are emotional creatures and therefore, the...\n",
            "--------------------------------------------------\n",
            "Rank 5 | [PID: 7131418] | Score: -1.3330\n",
            "Text:  If your cat follows you from room to room and hangs out wherever you are, itâ\u0080\u0099s a sign that sheâ\u0080\u0099s interested in you and wants to be where you are. ...\n",
            "--------------------------------------------------\n",
            "Rank 6 | [PID: 6322784] | Score: -2.3633\n",
            "Text:  Most of the cat muscles are similar in shape and location as they are in humans; however their function is somewhat different. Humanâ\u0080\u0099s muscles are m...\n",
            "--------------------------------------------------\n",
            "\n",
            "------------------------------\n",
            "Enter Search Query (or 'exit' to quit): being happy in life\n",
            "Enter Top-K results to return (default 10): 5\n",
            "Original Query: being happy in life\n",
            "Expanded Query: being happy in life is do, attention in If both whatever pay don't used Happiness you live\n",
            "\n",
            "Top 5 Results for: 'being happy in life'\n",
            "--------------------------------------------------\n",
            "Rank 1 | [PID: 3236815] | Score: 5.8516\n",
            "Text:  Happiness is used in both life evaluation, as in â\u0080\u009cHow happy are you with your life as a whole?â\u0080\u009d, and in emotional reports, as in â\u0080\u009cHow happy are ...\n",
            "--------------------------------------------------\n",
            "Rank 2 | [PID: 2102812] | Score: 4.7891\n",
            "Text:  According to Aristotle, happiness consists in achieving, through the course of a whole lifetime, all the goods â\u0080\u0094 health, wealth, knowledge, friends,...\n",
            "--------------------------------------------------\n",
            "Rank 3 | [PID: 2139898] | Score: 1.5518\n",
            "Text:  whatever you do, pay attention to it, and it will become happy. Very small thing like eating or breathing or walking, just pay attention, donâ\u0080\u0099t let ...\n",
            "--------------------------------------------------\n",
            "Rank 4 | [PID: 5241449] | Score: 0.8950\n",
            "Text:  Does happiness involve self-sacrifice? Does there need to be a meaning of life in order to find happiness? Or perhaps finding happiness is the meaning...\n",
            "--------------------------------------------------\n",
            "Rank 5 | [PID: 5450349] | Score: 0.2407\n",
            "Text:  Balanced and Healthy lifestyle is the life long effort to saturates whole our life with balanced and healthy activities by body, speech and mind in Ba...\n",
            "--------------------------------------------------\n",
            "\n",
            "------------------------------\n",
            "Enter Search Query (or 'exit' to quit): in which season mango is grown\n",
            "Enter Top-K results to return (default 10): 6\n",
            "Original Query: in which season mango is grown\n",
            "Expanded Query: in which season mango is grown Langra Langra. and spring, is There in Mango one of Season flower Mangoes Hawaii.\n",
            "\n",
            "Top 6 Results for: 'in which season mango is grown'\n",
            "--------------------------------------------------\n",
            "Rank 1 | [PID: 2415989] | Score: 5.4648\n",
            "Text:  Mangoes flower in spring, and the fruit matures from October through to April, depending on the variety and the location. The fruit is ready to pick w...\n",
            "--------------------------------------------------\n",
            "Rank 2 | [PID: 5354798] | Score: 3.0625\n",
            "Text:  Mango Season in Hawaii. There is nothing like mangoes when they are in season in Hawaii. When the trees are loaded with mangoes, itâ\u0080\u0099s a great time t...\n",
            "--------------------------------------------------\n",
            "Rank 3 | [PID: 6177747] | Score: 1.3252\n",
            "Text:  Langra. Langra is one of the most superior varieties of mango cultivated in the North India, especially Varanasi. Its greenish-yellow colour and soft,...\n",
            "--------------------------------------------------\n",
            "Rank 4 | [PID: 4834188] | Score: -1.8447\n",
            "Text:  The ideal soil texture for mango cultivation under irrigation is a sandy loam or loam (with a clay content of 15 to 25 %), but soils with a clay conte...\n",
            "--------------------------------------------------\n",
            "Rank 5 | [PID: 5354795] | Score: -3.8359\n",
            "Text:  Â©iStockphoto.com/YinYang. One of the most celebrated tropical fruits out there, mangoes are ripe for the picking in the next few months. Characterize...\n",
            "--------------------------------------------------\n",
            "Rank 6 | [PID: 5875442] | Score: -5.2188\n",
            "Text:  Cooler temperatures delay the ripening of the fruit. Check the mangoes for ripeness every one to two days during the ripening process. Press the mango...\n",
            "--------------------------------------------------\n",
            "\n",
            "------------------------------\n",
            "Enter Search Query (or 'exit' to quit): exit\n",
            "Exited from search system\n"
          ]
        }
      ],
      "source": [
        "def run_simple_cli():\n",
        "    print(\"-\"*50)\n",
        "    print(\"MS MARCO NEURAL SEARCH SYSTEM\")\n",
        "    print(\"Mode: Bi-Encoder Retrieval -> Cross-Encoder Re-Rank\")\n",
        "    print(\"-\"*50)\n",
        "\n",
        "    if 'bi_model' not in globals() or 'expert_index' not in globals():\n",
        "        print(\"ERROR: Models or Index not found. Please run the indexing cell above.\")\n",
        "        return\n",
        "\n",
        "    while True:\n",
        "        print(\"\\n\" + \"-\"*30)\n",
        "        query = input(\"Enter Search Query (or 'exit' to quit): \").strip()\n",
        "\n",
        "        if query.lower() in ['exit', 'quit', 'q']:\n",
        "            print(\"Exited from search system\")\n",
        "            break\n",
        "\n",
        "        if not query:\n",
        "            print(\"Empty query.\")\n",
        "            continue\n",
        "\n",
        "        k_input = input(\"Enter Top-K results to return (default 10): \").strip()\n",
        "        try:\n",
        "            k = int(k_input) if k_input else 10\n",
        "        except ValueError:\n",
        "            k = 10\n",
        "\n",
        "\n",
        "        # QUERY EXPANSION\n",
        "        print(f\"Original Query: {query}\")\n",
        "        # expanding query\n",
        "        expanded_query = expand_query_pseudo_relevance(query, bi_model, expert_index, subset_df)\n",
        "        print(f\"Expanded Query: {expanded_query}\")\n",
        "\n",
        "        # RETRIEVAL\n",
        "        # using expanded query\n",
        "        q_emb = bi_model.encode([expanded_query], convert_to_numpy=True, show_progress_bar=False).astype('float32')\n",
        "        faiss.normalize_L2(q_emb)\n",
        "\n",
        "        # Searching FAISS and retrieving top 40 for re-ranking\n",
        "        _, d_indices = expert_index.search(q_emb, k=40)\n",
        "\n",
        "        # Mapping Indices to Candidates\n",
        "        candidates = []\n",
        "        for idx in d_indices[0]:\n",
        "            if 0 <= idx < len(subset_df):\n",
        "                pid = str(subset_df.iloc[idx]['pid'])\n",
        "                if pid in pid_to_text:\n",
        "                    candidates.append({\n",
        "                        'pid': pid,\n",
        "                        'text': pid_to_text[pid]\n",
        "                    })\n",
        "\n",
        "        if not candidates:\n",
        "            print(\"No results found in the subset.\")\n",
        "            continue\n",
        "\n",
        "        # RE-RANKING\n",
        "        model_inputs = [[query, c['text']] for c in candidates]\n",
        "        scores = cross_model.predict(model_inputs, batch_size=64, show_progress_bar=False)\n",
        "\n",
        "        for i in range(len(candidates)):\n",
        "            candidates[i]['score'] = float(scores[i])\n",
        "\n",
        "        # Sorting by high-precision Cross-Encoder score\n",
        "        results = sorted(candidates, key=lambda x: x['score'], reverse=True)[:k]\n",
        "\n",
        "\n",
        "        # printing results\n",
        "        print(f\"\\nTop {len(results)} Results for: '{query}'\")\n",
        "        print(\"-\"*50)\n",
        "\n",
        "        for i, res in enumerate(results):\n",
        "            print(f\"Rank {i+1} | [PID: {res['pid']}] | Score: {res['score']:.4f}\")\n",
        "\n",
        "            clean_text = res['text'].replace('\\n', ' ').strip()\n",
        "            if len(clean_text) > 150:\n",
        "                snippet = clean_text[:150] + \"...\"\n",
        "            else:\n",
        "                snippet = clean_text\n",
        "\n",
        "            print(f\"Text:  {snippet}\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "run_simple_cli()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
